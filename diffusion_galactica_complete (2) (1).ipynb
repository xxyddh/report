{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Galactica to Diffusion Model - å®Œæ•´è®­ç»ƒç‰ˆ\n",
    "\n",
    "**ç¯å¢ƒ**: AutoDL GPUå®ä¾‹  \n",
    "**GPU**: RTX 5090 (32GB)  \n",
    "**è®ºæ–‡**: [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/abs/2410.17891)\n",
    "**æ•°æ®é›†**: ArXivè®ºæ–‡æ•°æ®é›†\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Notebookç»“æ„\n",
    "\n",
    "1. ç¯å¢ƒæ£€æŸ¥å’Œé…ç½®\n",
    "2. å¯¼å…¥åº“å’Œå®šä¹‰é…ç½®ç±»\n",
    "3. æ ¸å¿ƒDiffusionå‡½æ•°\n",
    "4. æ•°æ®é›†ç±»\n",
    "5. æ¨¡å‹åŒ…è£…ç±»\n",
    "6. æ¨¡å‹åŠ è½½\n",
    "7. **æ•°æ®å‡†å¤‡ï¼ˆArXivæ•°æ®é›†ï¼‰**\n",
    "8. è®­ç»ƒå¾ªç¯ï¼ˆåŒ…å«æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼‰\n",
    "9. è®­ç»ƒå¯è§†åŒ–\n",
    "10. ç”Ÿæˆæµ‹è¯•\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ç¯å¢ƒæ£€æŸ¥å’Œé…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ç¯å¢ƒæ£€æŸ¥\n",
      "============================================================\n",
      "\n",
      "âœ“ PyTorchç‰ˆæœ¬: 2.7.0+cu128\n",
      "âœ“ CUDAå¯ç”¨: True\n",
      "âœ“ GPU: NVIDIA GeForce RTX 5090\n",
      "âœ“ æ˜¾å­˜: 33.7 GB\n",
      "âœ“ CUDAç‰ˆæœ¬: 12.8\n",
      "\n",
      "============================================================\n",
      "âœ“ ç¯å¢ƒæ£€æŸ¥å®Œæˆ\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# å¿½ç•¥è­¦å‘Š\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ç¯å¢ƒæ£€æŸ¥\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nâœ“ PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"âœ“ CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ“ æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"âœ“ CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Š: GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUï¼ˆè®­ç»ƒä¼šå¾ˆæ…¢ï¼‰\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ ç¯å¢ƒæ£€æŸ¥å®Œæˆ\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ å¯¼å…¥åº“å’Œå®šä¹‰é…ç½®ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(\"âœ“ æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ é…ç½®å·²åŠ è½½\n",
      "\n",
      "é…ç½®æ‘˜è¦:\n",
      "  - è®¾å¤‡: cuda\n",
      "  - æ•°æ®è·¯å¾„: ./autodl-tmp/arxiv_train-1.jsonl\n",
      "  - Batch size: 8\n",
      "  - å­¦ä¹ ç‡: 0.0001\n",
      "  - æœ€å¤§åºåˆ—é•¿åº¦: 128\n",
      "  - æœ€å¤§è®­ç»ƒæ ·æœ¬: å…¨éƒ¨\n",
      "  - ä½¿ç”¨FP32: True\n"
     ]
    }
   ],
   "source": [
    "# é…ç½®ç±»\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"è®­ç»ƒé…ç½®å‚æ•°\"\"\"\n",
    "    \n",
    "    # æ¨¡å‹è·¯å¾„\n",
    "    model_path: str = \"/root/.cache/modelscope/hub/models/facebook/galactica-125m\"\n",
    "    \n",
    "    # æ•°æ®è·¯å¾„\n",
    "    data_path: str = \"./autodl-tmp/arxiv_train-1.jsonl\"  # ArXivå¤„ç†åçš„æ•°æ®\n",
    "    \n",
    "    # è®­ç»ƒå‚æ•°\n",
    "    batch_size: int = 8  # é™ä½batch sizeæé«˜ç¨³å®šæ€§\n",
    "    gradient_accumulation_steps: int = 4  # æ¢¯åº¦ç´¯ç§¯\n",
    "    num_epochs: int = 3\n",
    "    learning_rate: float = 1e-4  # é™ä½å­¦ä¹ ç‡\n",
    "    warmup_steps: int = 500\n",
    "    max_grad_norm: float = 0.5  # ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª\n",
    "    \n",
    "    # æ•°æ®å‚æ•°\n",
    "    max_length: int = 128\n",
    "    max_train_samples: int = None  # Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œå¯è®¾ç½®ä¸º10000è¿›è¡Œå¿«é€Ÿæµ‹è¯•\n",
    "    \n",
    "    # Diffusionå‚æ•°\n",
    "    mask_token_id: int = None  # å°†åœ¨åŠ è½½tokenizeråè‡ªåŠ¨è®¾ç½®\n",
    "    \n",
    "    # è®¾å¤‡å’Œè¾“å‡º\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    output_dir: str = \"/root/autodl-tmp/diffusion_galactica_checkpoints\"\n",
    "    save_steps: int = 1000\n",
    "    \n",
    "    # æ•°å€¼ç¨³å®šæ€§\n",
    "    use_fp32: bool = True  # ä½¿ç”¨FP32æé«˜æ•°å€¼ç¨³å®šæ€§\n",
    "    check_nan: bool = True  # å¯ç”¨NaNæ£€æµ‹\n",
    "\n",
    "config = TrainingConfig()\n",
    "print(\"âœ“ é…ç½®å·²åŠ è½½\")\n",
    "print(f\"\\né…ç½®æ‘˜è¦:\")\n",
    "print(f\"  - è®¾å¤‡: {config.device}\")\n",
    "print(f\"  - æ•°æ®è·¯å¾„: {config.data_path}\")\n",
    "print(f\"  - Batch size: {config.batch_size}\")\n",
    "print(f\"  - å­¦ä¹ ç‡: {config.learning_rate}\")\n",
    "print(f\"  - æœ€å¤§åºåˆ—é•¿åº¦: {config.max_length}\")\n",
    "print(f\"  - æœ€å¤§è®­ç»ƒæ ·æœ¬: {config.max_train_samples if config.max_train_samples else 'å…¨éƒ¨'}\")\n",
    "print(f\"  - ä½¿ç”¨FP32: {config.use_fp32}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ æ ¸å¿ƒDiffusionå‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Diffusionæ ¸å¿ƒå‡½æ•°å®šä¹‰å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "def forward_diffusion_step(x0, t, mask_token_id, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    å‰å‘diffusionè¿‡ç¨‹ï¼šæ ¹æ®æ—¶é—´æ­¥tå°†x0è½¬æ¢ä¸ºxt\n",
    "    \n",
    "    Args:\n",
    "        x0: åŸå§‹tokenåºåˆ— [batch_size, seq_len]\n",
    "        t: æ—¶é—´æ­¥ [batch_size] (èŒƒå›´0-1)\n",
    "        mask_token_id: mask tokençš„ID\n",
    "    \n",
    "    Returns:\n",
    "        xt: åŠ å™ªåçš„åºåˆ— [batch_size, seq_len]\n",
    "        mask: å“ªäº›ä½ç½®è¢«maskäº† [batch_size, seq_len]\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = x0.shape\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆç‹¬ç«‹çš„maskæ¦‚ç‡\n",
    "    mask_prob = t.unsqueeze(1).expand(-1, seq_len)\n",
    "    \n",
    "    # ç”Ÿæˆéšæœºmask\n",
    "    random_values = torch.rand(batch_size, seq_len, device=device)\n",
    "    mask = random_values < mask_prob\n",
    "    \n",
    "    # åº”ç”¨mask\n",
    "    xt = x0.clone()\n",
    "    xt[mask] = mask_token_id\n",
    "    \n",
    "    return xt, mask\n",
    "\n",
    "\n",
    "def compute_diffusion_loss(logits, x0, mask, t):\n",
    "    \"\"\"\n",
    "    è®¡ç®—diffusionæŸå¤±ï¼ˆå¸¦æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼‰\n",
    "    \n",
    "    Args:\n",
    "        logits: æ¨¡å‹è¾“å‡º [batch_size, seq_len, vocab_size]\n",
    "        x0: çœŸå®token [batch_size, seq_len]\n",
    "        mask: maskä½ç½® [batch_size, seq_len]\n",
    "        t: æ—¶é—´æ­¥ [batch_size]\n",
    "    \"\"\"\n",
    "    # æ£€æŸ¥è¾“å…¥æ•°å€¼ç¨³å®šæ€§\n",
    "    if torch.isnan(logits).any():\n",
    "        print(\"âš ï¸  è­¦å‘Š: logitsåŒ…å«NaN\")\n",
    "        logits = torch.nan_to_num(logits, nan=0.0)\n",
    "    \n",
    "    if torch.isinf(logits).any():\n",
    "        print(\"âš ï¸  è­¦å‘Š: logitsåŒ…å«Inf\")\n",
    "        logits = torch.nan_to_num(logits, posinf=1e4, neginf=-1e4)\n",
    "    \n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    \n",
    "    # åªè®¡ç®—è¢«maskä½ç½®çš„æŸå¤±\n",
    "    logits_flat = logits.reshape(-1, vocab_size)\n",
    "    targets_flat = x0.reshape(-1)\n",
    "    mask_flat = mask.reshape(-1)\n",
    "    \n",
    "    # è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼ˆä¸è¿›è¡Œreductionï¼‰\n",
    "    ce_loss = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
    "    \n",
    "    # åªä¿ç•™maskä½ç½®çš„æŸå¤±\n",
    "    ce_loss = ce_loss * mask_flat.float()\n",
    "    \n",
    "    # æ£€æŸ¥æŸå¤±çš„æ•°å€¼ç¨³å®šæ€§\n",
    "    if torch.isnan(ce_loss).any() or torch.isinf(ce_loss).any():\n",
    "        print(\"âš ï¸  è­¦å‘Š: æŸå¤±åŒ…å«NaNæˆ–Infï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡\")\n",
    "        return None\n",
    "    \n",
    "    # æ—¶é—´æ­¥åŠ æƒï¼ˆè¾ƒå°çš„tæƒé‡æ›´å¤§ï¼‰\n",
    "    if mask_flat.sum() > 0:\n",
    "        t_expanded = t.unsqueeze(1).expand(-1, seq_len).reshape(-1)\n",
    "        t_for_masked = t_expanded[mask_flat]\n",
    "        \n",
    "        # é¿å…é™¤ä»¥0\n",
    "        weights = 1.0 / (t_for_masked + 1e-8)\n",
    "        weighted_loss = ce_loss[mask_flat] * weights\n",
    "        \n",
    "        return weighted_loss.mean()\n",
    "    else:\n",
    "        return ce_loss.mean()\n",
    "\n",
    "\n",
    "print(\"âœ“ Diffusionæ ¸å¿ƒå‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ æ•°æ®é›†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ DiffusionDatasetç±»å®šä¹‰å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class DiffusionDataset(Dataset):\n",
    "    \"\"\"Diffusionè®­ç»ƒæ•°æ®é›†\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "print(\"âœ“ DiffusionDatasetç±»å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ æ¨¡å‹åŒ…è£…ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ DiffusionGalacticaç±»å®šä¹‰å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "class DiffusionGalactica(nn.Module):\n",
    "    \"\"\"åŒ…è£…Galacticaæ¨¡å‹ä»¥æ”¯æŒdiffusionè®­ç»ƒ\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.config = base_model.config\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Args:\n",
    "            input_ids: token IDs [batch_size, seq_len]\n",
    "            attention_mask: attention mask [batch_size, seq_len]\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        return outputs.logits\n",
    "\n",
    "print(\"âœ“ DiffusionGalacticaç±»å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ æ¨¡å‹åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "åŠ è½½æ¨¡å‹\n",
      "============================================================\n",
      "\n",
      "1. åŠ è½½Galactica-125Mæ¨¡å‹...\n",
      "   è·¯å¾„: /root/.cache/modelscope/hub/models/facebook/galactica-125m\n",
      "   âœ“ æ·»åŠ æ–°çš„padding token: [PAD]\n",
      "   Pad token: [PAD]\n",
      "   Pad token ID: 50000\n",
      "   Mask token ID: 50000\n",
      "   è¯è¡¨å¤§å°: 50001\n",
      "   âœ“ mask_token_idéªŒè¯é€šè¿‡\n",
      "   âœ“ è°ƒæ•´æ¨¡å‹è¯è¡¨å¤§å°ä»¥é€‚åº”æ–°çš„pad token\n",
      "\n",
      "âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\n",
      "   æ€»å‚æ•°: 125,031,168\n",
      "   å¯è®­ç»ƒå‚æ•°: 125,031,168\n",
      "   è®¾å¤‡: cuda:0\n",
      "   æ•°æ®ç±»å‹: torch.float32\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"åŠ è½½æ¨¡å‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n1. åŠ è½½Galactica-125Mæ¨¡å‹...\")\n",
    "print(f\"   è·¯å¾„: {config.model_path}\")\n",
    "\n",
    "# æ£€æŸ¥æ¨¡å‹è·¯å¾„\n",
    "if not os.path.exists(config.model_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {config.model_path}\\n\"\n",
    "        f\"è¯·ç¡®ä¿å·²é€šè¿‡ModelScopeä¸‹è½½æ¨¡å‹\"\n",
    "    )\n",
    "\n",
    "# åŠ è½½tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_path)\n",
    "\n",
    "# è®¾ç½®padding tokenï¼ˆå…³é”®ä¿®å¤ï¼‰\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"   âœ“ è®¾ç½®pad_tokenä¸ºeos_token: {tokenizer.eos_token}\")\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        print(\"   âœ“ æ·»åŠ æ–°çš„padding token: [PAD]\")\n",
    "\n",
    "print(f\"   Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"   Pad token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# âœ… è®¾ç½®mask_token_idï¼ˆä¿®å¤CUDAç´¢å¼•é”™è¯¯ï¼‰\n",
    "config.mask_token_id = tokenizer.pad_token_id\n",
    "print(f\"   Mask token ID: {config.mask_token_id}\")\n",
    "print(f\"   è¯è¡¨å¤§å°: {len(tokenizer)}\")\n",
    "\n",
    "# éªŒè¯mask_token_idåœ¨æœ‰æ•ˆèŒƒå›´å†…\n",
    "assert config.mask_token_id is not None, \"mask_token_idæœªè®¾ç½®\"\n",
    "assert config.mask_token_id < len(tokenizer), \\\n",
    "    f\"mask_token_id ({config.mask_token_id}) è¶…å‡ºè¯è¡¨å¤§å° ({len(tokenizer)})\"\n",
    "print(\"   âœ“ mask_token_idéªŒè¯é€šè¿‡\")\n",
    "\n",
    "# åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "dtype = torch.float32 if config.use_fp32 else torch.float16\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_path,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=None  # æ‰‹åŠ¨ç®¡ç†è®¾å¤‡\n",
    ")\n",
    "\n",
    "# å¦‚æœæ·»åŠ äº†æ–°çš„pad tokenï¼Œè°ƒæ•´è¯è¡¨å¤§å°\n",
    "if tokenizer.pad_token == '[PAD]':\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"   âœ“ è°ƒæ•´æ¨¡å‹è¯è¡¨å¤§å°ä»¥é€‚åº”æ–°çš„pad token\")\n",
    "\n",
    "# åŒ…è£…ä¸ºdiffusionæ¨¡å‹å¹¶ç§»åˆ°è®¾å¤‡\n",
    "model = DiffusionGalactica(base_model)\n",
    "model = model.to(config.device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nâœ“ æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "print(f\"   æ€»å‚æ•°: {total_params:,}\")\n",
    "print(f\"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
    "print(f\"   è®¾å¤‡: {next(model.parameters()).device}\")\n",
    "print(f\"   æ•°æ®ç±»å‹: {next(model.parameters()).dtype}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ æ•°æ®å‡†å¤‡ï¼ˆArXivæ•°æ®é›†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "æ•°æ®å‡†å¤‡\n",
      "============================================================\n",
      "\n",
      "1. åŠ è½½è®­ç»ƒæ•°æ®...\n",
      "ä»JSONLæ–‡ä»¶åŠ è½½: autodl-tmp/arxiv_train-1.jsonl\n",
      "\n",
      "âœ“ åŠ è½½å®Œæˆ: 32094 æ¡è®­ç»ƒæ ·æœ¬\n",
      "\n",
      "æ•°æ®ç»Ÿè®¡:\n",
      "  æ ·æœ¬æ•°: 32094\n",
      "  å¹³å‡é•¿åº¦: 33456 å­—ç¬¦\n",
      "  æœ€å¤§é•¿åº¦: 657995 å­—ç¬¦\n",
      "  æœ€å°é•¿åº¦: 101 å­—ç¬¦\n",
      "\n",
      "ç¬¬ä¸€ä¸ªæ ·æœ¬é¢„è§ˆ (å‰200å­—ç¬¦):\n",
      "------------------------------------------------------------\n",
      "additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when comp...\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. åˆ›å»ºæ•°æ®é›†...\n",
      "3. åˆ›å»ºDataLoader...\n",
      "\n",
      "âœ“ æ•°æ®å‡†å¤‡å®Œæˆ\n",
      "  è®­ç»ƒæ ·æœ¬æ•°: 32094\n",
      "  Batchå¤§å°: 8\n",
      "  Batchæ•°é‡: 4012\n",
      "  æ¯ä¸ªepochæ­¥æ•°: 4012\n",
      "  ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯: 4 æ­¥\n",
      "  æœ‰æ•ˆbatch size: 32\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"æ•°æ®å‡†å¤‡\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def load_training_data(data_path, max_samples=None):\n",
    "    \"\"\"\n",
    "    åŠ è½½å¤„ç†å¥½çš„è®­ç»ƒæ•°æ®\n",
    "    \n",
    "    Args:\n",
    "        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ (.txt æˆ– .jsonl)\n",
    "        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨ï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        texts: æ–‡æœ¬åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    data_path = Path(data_path)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}\\n\"\n",
    "            f\"è¯·å…ˆè¿è¡Œ process_arxiv_dataset.py å¤„ç†æ•°æ®\"\n",
    "        )\n",
    "    \n",
    "    texts = []\n",
    "    \n",
    "    # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹å¼\n",
    "    if data_path.suffix == '.txt':\n",
    "        print(f\"ä»æ–‡æœ¬æ–‡ä»¶åŠ è½½: {data_path}\")\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if max_samples and i >= max_samples:\n",
    "                    break\n",
    "                text = line.strip()\n",
    "                if text:  # è·³è¿‡ç©ºè¡Œ\n",
    "                    texts.append(text)\n",
    "    \n",
    "    elif data_path.suffix == '.jsonl':\n",
    "        print(f\"ä»JSONLæ–‡ä»¶åŠ è½½: {data_path}\")\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if max_samples and i >= max_samples:\n",
    "                    break\n",
    "                data = json.loads(line)\n",
    "                texts.append(data['text'])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_path.suffix}ï¼Œè¯·ä½¿ç”¨ .txt æˆ– .jsonl\")\n",
    "    \n",
    "    return texts\n",
    "\n",
    "\n",
    "# åŠ è½½è®­ç»ƒæ•°æ®\n",
    "print(\"\\n1. åŠ è½½è®­ç»ƒæ•°æ®...\")\n",
    "try:\n",
    "    train_texts = load_training_data(\n",
    "        data_path=config.data_path,\n",
    "        max_samples=config.max_train_samples\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ åŠ è½½å®Œæˆ: {len(train_texts)} æ¡è®­ç»ƒæ ·æœ¬\")\n",
    "    \n",
    "    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯\n",
    "    avg_len = sum(len(t) for t in train_texts) / len(train_texts) if train_texts else 0\n",
    "    max_len = max(len(t) for t in train_texts) if train_texts else 0\n",
    "    min_len = min(len(t) for t in train_texts) if train_texts else 0\n",
    "    \n",
    "    print(f\"\\næ•°æ®ç»Ÿè®¡:\")\n",
    "    print(f\"  æ ·æœ¬æ•°: {len(train_texts)}\")\n",
    "    print(f\"  å¹³å‡é•¿åº¦: {avg_len:.0f} å­—ç¬¦\")\n",
    "    print(f\"  æœ€å¤§é•¿åº¦: {max_len} å­—ç¬¦\")\n",
    "    print(f\"  æœ€å°é•¿åº¦: {min_len} å­—ç¬¦\")\n",
    "    \n",
    "    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„é¢„è§ˆ\n",
    "    if train_texts:\n",
    "        print(f\"\\nç¬¬ä¸€ä¸ªæ ·æœ¬é¢„è§ˆ (å‰200å­—ç¬¦):\")\n",
    "        print(\"-\" * 60)\n",
    "        print(train_texts[0][:200] + \"...\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nâŒ é”™è¯¯: {e}\")\n",
    "    print(\"\\nè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å‡†å¤‡æ•°æ®:\")\n",
    "    print(\"  1. ç¡®ä¿å·²ä¸‹è½½arxivæ•°æ®é›†çš„parquetæ–‡ä»¶\")\n",
    "    print(\"  2. è¿è¡Œ: python process_arxiv_dataset.py /path/to/parquet/files\")\n",
    "    print(\"  3. ç¡®ä¿ç”Ÿæˆäº† ./processed_data/arxiv_train.txt æ–‡ä»¶\")\n",
    "    raise\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†\n",
    "print(\"\\n2. åˆ›å»ºæ•°æ®é›†...\")\n",
    "train_dataset = DiffusionDataset(\n",
    "    texts=train_texts,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.max_length\n",
    ")\n",
    "\n",
    "# åˆ›å»ºDataLoader\n",
    "print(\"3. åˆ›å»ºDataLoader...\")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if config.device == \"cuda\" else False\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ æ•°æ®å‡†å¤‡å®Œæˆ\")\n",
    "print(f\"  è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}\")\n",
    "print(f\"  Batchå¤§å°: {config.batch_size}\")\n",
    "print(f\"  Batchæ•°é‡: {len(train_dataloader)}\")\n",
    "print(f\"  æ¯ä¸ªepochæ­¥æ•°: {len(train_dataloader)}\")\n",
    "print(f\"  ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯: {config.gradient_accumulation_steps} æ­¥\")\n",
    "print(f\"  æœ‰æ•ˆbatch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ è®­ç»ƒå¾ªç¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "å¼€å§‹è®­ç»ƒ\n",
      "============================================================\n",
      "\n",
      "è®­ç»ƒé…ç½®:\n",
      "  æ€»æ­¥æ•°: 12036\n",
      "  é¢„çƒ­æ­¥æ•°: 500\n",
      "  ä¿å­˜é—´éš”: 1000 æ­¥\n",
      "  æ¢¯åº¦è£å‰ª: 0.5\n",
      "\n",
      "\n",
      "============================================================\n",
      "Epoch 1/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d9d68ec31045dfb38675c030018ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/4012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-1000\n",
      "ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°: loss=9.3457\n",
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-1000\n",
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-1000\n",
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-1000\n",
      "ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°: loss=9.3385\n",
      "\n",
      "Epoch 1 å®Œæˆ\n",
      "  å¹³å‡Loss: 9.7569\n",
      "  æœ€ä½³Loss: 9.3385\n",
      "  å®Œæˆæ­¥æ•°: 1003\n",
      "\n",
      "============================================================\n",
      "Epoch 2/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4379daebdd5c409bb693a2af0e86a04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/4012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-2000\n",
      "ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°: loss=8.9585\n",
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-2000\n",
      "ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°: loss=8.9140\n",
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-2000\n",
      "ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°: loss=8.9025\n",
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-2000\n",
      "ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°: loss=8.8759\n",
      "\n",
      "Epoch 2 å®Œæˆ\n",
      "  å¹³å‡Loss: 9.2432\n",
      "  æœ€ä½³Loss: 8.8759\n",
      "  å®Œæˆæ­¥æ•°: 2006\n",
      "\n",
      "============================================================\n",
      "Epoch 3/3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2375fd193e4e24b6cee9042577e051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/4012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-3000\n",
      "ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°: loss=8.7752\n",
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-3000\n",
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-3000\n",
      "\n",
      "ğŸ’¾ Checkpointä¿å­˜: /root/autodl-tmp/diffusion_galactica_checkpoints/checkpoint-3000\n",
      "\n",
      "Epoch 3 å®Œæˆ\n",
      "  å¹³å‡Loss: 9.0166\n",
      "  æœ€ä½³Loss: 8.7752\n",
      "  å®Œæˆæ­¥æ•°: 3009\n",
      "\n",
      "============================================================\n",
      "âœ… è®­ç»ƒå®Œæˆï¼\n",
      "============================================================\n",
      "æ€»è®­ç»ƒæ—¶é—´: 0.27 å°æ—¶\n",
      "æ€»æ­¥æ•°: 3009\n",
      "æœ€ä½³Loss: 8.7752\n",
      "æœ€ç»ˆLoss: 8.6378\n",
      "æ¨¡å‹ä¿å­˜ä½ç½®: /root/autodl-tmp/diffusion_galactica_checkpoints\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"å¼€å§‹è®­ç»ƒ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# è®¾ç½®ä¼˜åŒ–å™¨\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "total_steps = len(train_dataloader) * config.num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=config.warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nè®­ç»ƒé…ç½®:\")\n",
    "print(f\"  æ€»æ­¥æ•°: {total_steps}\")\n",
    "print(f\"  é¢„çƒ­æ­¥æ•°: {config.warmup_steps}\")\n",
    "print(f\"  ä¿å­˜é—´éš”: {config.save_steps} æ­¥\")\n",
    "print(f\"  æ¢¯åº¦è£å‰ª: {config.max_grad_norm}\")\n",
    "print()\n",
    "\n",
    "# è®­ç»ƒå¾ªç¯\n",
    "model.train()\n",
    "train_losses = []\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    epoch_losses = []\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        try:\n",
    "            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡\n",
    "            input_ids = batch[\"input_ids\"].to(config.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(config.device)\n",
    "            \n",
    "            # é‡‡æ ·æ—¶é—´æ­¥\n",
    "            batch_size = input_ids.shape[0]\n",
    "            t = torch.rand(batch_size, device=config.device)\n",
    "            \n",
    "            # å‰å‘diffusion\n",
    "            xt, mask = forward_diffusion_step(\n",
    "                input_ids,\n",
    "                t,\n",
    "                config.mask_token_id,\n",
    "                device=config.device\n",
    "            )\n",
    "            \n",
    "            # æ¨¡å‹å‰å‘ä¼ æ’­\n",
    "            logits = model(xt, attention_mask=attention_mask)\n",
    "            \n",
    "            # è®¡ç®—æŸå¤±\n",
    "            loss = compute_diffusion_loss(logits, input_ids, mask, t)\n",
    "            \n",
    "            # è·³è¿‡å¼‚å¸¸æ‰¹æ¬¡\n",
    "            if loss is None:\n",
    "                print(f\"\\nâš ï¸  æ­¥æ•° {global_step}: è·³è¿‡å¼‚å¸¸æ‰¹æ¬¡\")\n",
    "                continue\n",
    "            \n",
    "            # æ¢¯åº¦ç´¯ç§¯\n",
    "            loss = loss / config.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # æ¯éš”gradient_accumulation_stepsæ›´æ–°ä¸€æ¬¡\n",
    "            if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
    "                # æ¢¯åº¦è£å‰ª\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(),\n",
    "                    config.max_grad_norm\n",
    "                )\n",
    "                \n",
    "                # æ›´æ–°å‚æ•°\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                global_step += 1\n",
    "            \n",
    "            # è®°å½•æŸå¤±\n",
    "            current_loss = loss.item() * config.gradient_accumulation_steps\n",
    "            train_losses.append(current_loss)\n",
    "            epoch_losses.append(current_loss)\n",
    "            \n",
    "            # æ›´æ–°è¿›åº¦æ¡\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{current_loss:.4f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "            \n",
    "            # ä¿å­˜checkpoint\n",
    "            if global_step % config.save_steps == 0 and global_step > 0:\n",
    "                checkpoint_path = os.path.join(\n",
    "                    config.output_dir,\n",
    "                    f\"checkpoint-{global_step}\"\n",
    "                )\n",
    "                os.makedirs(checkpoint_path, exist_ok=True)\n",
    "                \n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'global_step': global_step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'loss': current_loss,\n",
    "                    'config': config\n",
    "                }, os.path.join(checkpoint_path, 'pytorch_model.bin'))\n",
    "                \n",
    "                print(f\"\\nğŸ’¾ Checkpointä¿å­˜: {checkpoint_path}\")\n",
    "                \n",
    "                # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "                avg_loss = sum(epoch_losses[-100:]) / min(100, len(epoch_losses))\n",
    "                if avg_loss < best_loss:\n",
    "                    best_loss = avg_loss\n",
    "                    best_model_path = os.path.join(config.output_dir, \"best_model\")\n",
    "                    os.makedirs(best_model_path, exist_ok=True)\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'loss': best_loss\n",
    "                    }, os.path.join(best_model_path, 'pytorch_model.bin'))\n",
    "                    print(f\"ğŸ† æœ€ä½³æ¨¡å‹æ›´æ–°: loss={best_loss:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Epochæ€»ç»“\n",
    "    if epoch_losses:\n",
    "        avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        print(f\"\\nEpoch {epoch + 1} å®Œæˆ\")\n",
    "        print(f\"  å¹³å‡Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"  æœ€ä½³Loss: {best_loss:.4f}\")\n",
    "        print(f\"  å®Œæˆæ­¥æ•°: {global_step}\")\n",
    "\n",
    "# è®­ç»ƒç»“æŸ\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… è®­ç»ƒå®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "print(f\"æ€»è®­ç»ƒæ—¶é—´: {elapsed_time/3600:.2f} å°æ—¶\")\n",
    "print(f\"æ€»æ­¥æ•°: {global_step}\")\n",
    "print(f\"æœ€ä½³Loss: {best_loss:.4f}\")\n",
    "print(f\"æœ€ç»ˆLoss: {train_losses[-1]:.4f}\" if train_losses else \"N/A\")\n",
    "print(f\"æ¨¡å‹ä¿å­˜ä½ç½®: {config.output_dir}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ è®­ç»ƒå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿\n",
    "if len(train_losses) > 0:\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # åŸå§‹lossæ›²çº¿\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, alpha=0.6, linewidth=0.8)\n",
    "    plt.xlabel('è®­ç»ƒæ­¥æ•°', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('è®­ç»ƒæŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # å¹³æ»‘lossæ›²çº¿\n",
    "    plt.subplot(1, 2, 2)\n",
    "    window = min(100, len(train_losses) // 10)\n",
    "    if len(train_losses) > window:\n",
    "        smoothed = np.convolve(train_losses, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(smoothed, linewidth=2, color='#2E86AB')\n",
    "        plt.xlabel('è®­ç»ƒæ­¥æ•°', fontsize=12)\n",
    "        plt.ylabel('å¹³æ»‘Loss', fontsize=12)\n",
    "        plt.title(f'è®­ç»ƒæŸå¤± (ç§»åŠ¨å¹³å‡, window={window})', fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜å›¾ç‰‡\n",
    "    loss_plot_path = os.path.join(config.output_dir, 'training_loss.png')\n",
    "    plt.savefig(loss_plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {loss_plot_path}\")\n",
    "    \n",
    "    # è®­ç»ƒç»Ÿè®¡\n",
    "    print(\"\\nè®­ç»ƒç»Ÿè®¡:\")\n",
    "    print(f\"  æœ€ç»ˆLoss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  æœ€ä½³Loss: {best_loss:.4f}\")\n",
    "    if len(train_losses) > 1:\n",
    "        improvement = (train_losses[0] - train_losses[-1]) / train_losses[0] * 100\n",
    "        print(f\"  Lossé™ä½: {improvement:.1f}%\")\n",
    "    print(f\"  æ€»æ­¥æ•°: {len(train_losses)}\")\n",
    "else:\n",
    "    print(\"âš ï¸  æ²¡æœ‰è®­ç»ƒæŸå¤±æ•°æ®å¯è§†åŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ ç”Ÿæˆæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_diffusion(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=\"\",\n",
    "    num_steps=64,\n",
    "    max_length=128,\n",
    "    temperature=0.9,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨diffusionæ¨¡å‹ç”Ÿæˆæ–‡æœ¬\n",
    "    \n",
    "    Args:\n",
    "        model: è®­ç»ƒå¥½çš„diffusionæ¨¡å‹\n",
    "        tokenizer: tokenizer\n",
    "        prompt: æç¤ºæ–‡æœ¬\n",
    "        num_steps: åå‘diffusionæ­¥æ•°\n",
    "        max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦\n",
    "        temperature: é‡‡æ ·æ¸©åº¦\n",
    "        device: è®¾å¤‡\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    mask_token_id = config.mask_token_id\n",
    "    \n",
    "    # ç¼–ç prompt\n",
    "    if prompt:\n",
    "        prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        prompt_len = len(prompt_ids)\n",
    "    else:\n",
    "        prompt_ids = [tokenizer.bos_token_id] if tokenizer.bos_token_id else [0]\n",
    "        prompt_len = 1\n",
    "    \n",
    "    # åˆå§‹åŒ–åºåˆ—ï¼ˆå…¨maskï¼‰\n",
    "    total_len = min(max_length, 512)\n",
    "    xt = torch.full((1, total_len), mask_token_id, dtype=torch.long, device=device)\n",
    "    xt[0, :prompt_len] = torch.tensor(prompt_ids[:prompt_len], device=device)\n",
    "    \n",
    "    # åå‘diffusionè¿‡ç¨‹\n",
    "    with torch.no_grad():\n",
    "        for step in tqdm(reversed(range(1, num_steps + 1)), desc=\"ç”Ÿæˆä¸­\", leave=False):\n",
    "            t_current = step / num_steps\n",
    "            t_prev = (step - 1) / num_steps\n",
    "            \n",
    "            # æ¨¡å‹é¢„æµ‹\n",
    "            logits = model(xt)\n",
    "            \n",
    "            # æ•°å€¼ç¨³å®šæ€§å¤„ç†\n",
    "            if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "                logits = torch.nan_to_num(logits, nan=0.0, posinf=1e4, neginf=-1e4)\n",
    "            \n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # è®¡ç®—æ¦‚ç‡å¹¶é‡‡æ ·\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # æ£€æŸ¥æ¦‚ç‡æœ‰æ•ˆæ€§\n",
    "            if torch.isnan(probs).any() or torch.isinf(probs).any():\n",
    "                vocab_size = probs.shape[-1]\n",
    "                probs = torch.ones_like(probs) / vocab_size\n",
    "            \n",
    "            try:\n",
    "                x0_pred = torch.multinomial(probs.view(-1, probs.shape[-1]), 1).view(1, -1)\n",
    "            except:\n",
    "                x0_pred = torch.argmax(probs, dim=-1)\n",
    "            \n",
    "            # æ›´æ–°åºåˆ—\n",
    "            alpha_t = 1 - t_current\n",
    "            alpha_s = 1 - t_prev\n",
    "            \n",
    "            is_masked = (xt[0] == mask_token_id)\n",
    "            jump_prob = (alpha_s - alpha_t) / (1 - alpha_t + 1e-8) if step > 1 else 1.0\n",
    "            \n",
    "            should_jump = torch.rand(total_len, device=device) < jump_prob\n",
    "            update_mask = is_masked & should_jump\n",
    "            \n",
    "            xt[0, update_mask] = x0_pred[0, update_mask]\n",
    "            \n",
    "            # ä¿æŒpromptä¸å˜\n",
    "            xt[0, :prompt_len] = torch.tensor(prompt_ids[:prompt_len], device=device)\n",
    "    \n",
    "    # è§£ç \n",
    "    generated_ids = xt[0].cpu().tolist()\n",
    "    generated_ids = [tid for tid in generated_ids if tid != mask_token_id]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "print(\"âœ“ ç”Ÿæˆå‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆæµ‹è¯•\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ ç”Ÿæˆæµ‹è¯•\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_prompts = [\n",
    "    \"The theory of relativity\",\n",
    "    \"Artificial intelligence\",\n",
    "    \"Quantum mechanics\",\n",
    "    \"Climate change\",\n",
    "    \"Deep learning\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{i}. Prompt: '{prompt}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        generated = sample_diffusion(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt=prompt,\n",
    "            num_steps=64,\n",
    "            max_length=128,\n",
    "            temperature=0.9,\n",
    "            device=config.device\n",
    "        )\n",
    "        print(f\"ç”Ÿæˆ: {generated}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ç”Ÿæˆæµ‹è¯•å®Œæˆ\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ä½¿ç”¨è¯´æ˜\n",
    "\n",
    "### æ•°æ®å‡†å¤‡\n",
    "\n",
    "åœ¨è¿è¡Œæ­¤notebookä¹‹å‰ï¼Œè¯·ç¡®ä¿:\n",
    "\n",
    "1. **ä¸‹è½½ArXivæ•°æ®é›†**\n",
    "   - ä»Hugging Faceä¸‹è½½arxiv-summarizationæ•°æ®é›†çš„parquetæ–‡ä»¶\n",
    "\n",
    "2. **å¤„ç†æ•°æ®**\n",
    "   ```bash\n",
    "   python process_arxiv_dataset.py /path/to/parquet/files\n",
    "   ```\n",
    "\n",
    "3. **ç¡®è®¤æ•°æ®æ–‡ä»¶å­˜åœ¨**\n",
    "   - ç¡®ä¿ `./processed_data/arxiv_train.txt` æ–‡ä»¶å·²ç”Ÿæˆ\n",
    "\n",
    "### è®­ç»ƒé…ç½®\n",
    "\n",
    "**å¿«é€Ÿæµ‹è¯•** (åœ¨ç¬¬2èŠ‚ä¿®æ”¹config):\n",
    "```python\n",
    "config.max_train_samples = 1000\n",
    "config.batch_size = 4\n",
    "config.max_length = 64\n",
    "config.num_epochs = 1\n",
    "```\n",
    "\n",
    "**æ­£å¼è®­ç»ƒ**:\n",
    "```python\n",
    "config.max_train_samples = None  # ä½¿ç”¨å…¨éƒ¨æ•°æ®\n",
    "config.batch_size = 8\n",
    "config.max_length = 128\n",
    "config.num_epochs = 3\n",
    "```\n",
    "\n",
    "### è®­ç»ƒç›‘æ§\n",
    "\n",
    "- è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä¿å­˜checkpoint\n",
    "- æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨ `{output_dir}/best_model/`\n",
    "- è®­ç»ƒæ›²çº¿ä¿å­˜åœ¨ `{output_dir}/training_loss.png`\n",
    "\n",
    "### å¸¸è§é—®é¢˜\n",
    "\n",
    "1. **å†…å­˜ä¸è¶³**: å‡å°batch_sizeæˆ–max_length\n",
    "2. **NaN/Infé”™è¯¯**: å·²æœ‰æ•°å€¼ç¨³å®šæ€§å¤„ç†ï¼Œä¼šè‡ªåŠ¨è·³è¿‡å¼‚å¸¸æ‰¹æ¬¡\n",
    "3. **è®­ç»ƒå¤ªæ…¢**: å¯ä»¥å…ˆç”¨å°æ•°æ®é›†æµ‹è¯•ï¼ˆè®¾ç½®max_train_samplesï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
